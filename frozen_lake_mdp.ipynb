{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c04d236",
   "metadata": {},
   "source": [
    "## Universidad del Valle de Guatemala  \n",
    "## Departamento de Ciencias de la Computación  \n",
    "## Inteligencia Artificial - sección 10  \n",
    "\n",
    "### Hoja de Trabajo 2\n",
    "\n",
    "#### Nadissa Vela - 23764  \n",
    "#### Cristian Túnchez - 231359\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede6972e",
   "metadata": {},
   "source": [
    "# Task 2 - Implementación Frozen Lake en Python\n",
    "\n",
    "El agente debe cruzar un lago congelado representado por un grid de 4x4 donde:\n",
    "- **S (Start):** Posición inicial\n",
    "- **F (Frozen):** Camino seguro\n",
    "- **H (Hole):** Agujero - Termina el juego con recompensa 0\n",
    "- **G (Goal):** Meta - Recompensa +1\n",
    "\n",
    "**Dinámica Estocástica:**  \n",
    "El hielo es resbaladizo. Al tomar una acción, hay:\n",
    "- 1/3 de probabilidad de moverse en la dirección deseada\n",
    "- 1/3 de probabilidad de moverse a cada dirección perpendicular\n",
    "\n",
    "Ejemplo: Si intenta ir Norte, puede terminar en Norte, Este u Oeste con igual probabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea840ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de bibliotecas permitidas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4c036b",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2.1 - Modelado del MDP\n",
    "\n",
    "Implementaremos una clase `FrozenLakeMDP` que modele completamente el problema como un Proceso de Decisión de Markov.\n",
    "\n",
    "### Componentes del MDP\n",
    "\n",
    "Según la teoría, un MDP se define por:\n",
    "1. **Estados (S):** Conjunto de todos los estados posibles\n",
    "2. **Estado inicial (s_start):** Estado donde comienza el agente\n",
    "3. **Acciones (Actions(s)):** Acciones disponibles en cada estado\n",
    "4. **Función de transición (T(s, a, s')):** Probabilidad de llegar a s' al realizar acción a en estado s\n",
    "5. **Función de recompensa (Reward(s, a, s')):** Recompensa recibida por la transición\n",
    "6. **Estados finales (IsEnd(s)):** Indica si un estado es terminal\n",
    "7. **Factor de descuento (γ):** Importancia de recompensas futuras (usaremos γ=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5babc3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenLakeMDP:\n",
    "    \"\"\"\n",
    "    Implementación del problema Frozen Lake como un Markov Decision Process.\n",
    "    \n",
    "    Grid 4x4 con numeración de estados:\n",
    "    [ 0  1  2  3]\n",
    "    [ 4  5  6  7]\n",
    "    [ 8  9 10 11]\n",
    "    [12 13 14 15]\n",
    "    \n",
    "    Mapa del lago:\n",
    "    S F F F\n",
    "    F H F H\n",
    "    F F F H\n",
    "    H F F G\n",
    "    \n",
    "    Donde:\n",
    "    S = Start (estado 0)\n",
    "    F = Frozen (camino seguro)\n",
    "    H = Hole (agujero, estado terminal)\n",
    "    G = Goal (meta, estado terminal)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, gamma=0.9):\n",
    "        \"\"\"\n",
    "        Inicializa el MDP del Frozen Lake.\n",
    "        \n",
    "        Args:\n",
    "            gamma (float): Factor de descuento γ ∈ [0,1] para recompensas futuras\n",
    "        \"\"\"\n",
    "        # Factor de descuento γ\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Dimensiones del grid\n",
    "        self.rows = 4\n",
    "        self.cols = 4\n",
    "        self.n_states = self.rows * self.cols  # 16 estados (0-15)\n",
    "        \n",
    "        # Definición del mapa del lago (usando el layout estándar de Frozen Lake)\n",
    "        # S: Start, F: Frozen, H: Hole, G: Goal\n",
    "        self.lake_map = [\n",
    "            ['S', 'F', 'F', 'F'],\n",
    "            ['F', 'H', 'F', 'H'],\n",
    "            ['F', 'F', 'F', 'H'],\n",
    "            ['H', 'F', 'F', 'G']\n",
    "        ]\n",
    "        \n",
    "        # Estado inicial\n",
    "        self.start_state = 0  # Esquina superior izquierda\n",
    "        \n",
    "        # Estados terminales (Holes y Goal)\n",
    "        # Holes: 5, 7, 11, 12\n",
    "        # Goal: 15\n",
    "        self.hole_states = {5, 7, 11, 12}\n",
    "        self.goal_state = 15\n",
    "        self.terminal_states = self.hole_states | {self.goal_state}\n",
    "        \n",
    "        # Definición de acciones: 0=Norte, 1=Sur, 2=Este, 3=Oeste\n",
    "        self.actions = [0, 1, 2, 3]\n",
    "        self.action_names = ['Norte', 'Sur', 'Este', 'Oeste']\n",
    "        self.action_symbols = ['↑', '↓', '→', '←']\n",
    "        \n",
    "        # Movimientos correspondientes a cada acción\n",
    "        # (delta_row, delta_col)\n",
    "        self.action_deltas = {\n",
    "            0: (-1, 0),  # Norte: arriba\n",
    "            1: (1, 0),   # Sur: abajo\n",
    "            2: (0, 1),   # Este: derecha\n",
    "            3: (0, -1)   # Oeste: izquierda\n",
    "        }\n",
    "        \n",
    "        # Precalcular las direcciones perpendiculares para cada acción\n",
    "        # Para modelar la estocasticidad del hielo resbaladizo\n",
    "        self.perpendicular_actions = {\n",
    "            0: [2, 3],  # Norte -> perpendiculares son Este y Oeste\n",
    "            1: [2, 3],  # Sur -> perpendiculares son Este y Oeste\n",
    "            2: [0, 1],  # Este -> perpendiculares son Norte y Sur\n",
    "            3: [0, 1]   # Oeste -> perpendiculares son Norte y Sur\n",
    "        }\n",
    "    \n",
    "    def state_to_position(self, state):\n",
    "        \"\"\"\n",
    "        Convierte un número de estado (0-15) a posición (row, col) en el grid.\n",
    "        \n",
    "        Args:\n",
    "            state (int): Número de estado\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (row, col)\n",
    "        \"\"\"\n",
    "        row = state // self.cols\n",
    "        col = state % self.cols\n",
    "        return row, col\n",
    "    \n",
    "    def position_to_state(self, row, col):\n",
    "        \"\"\"\n",
    "        Convierte una posición (row, col) a número de estado.\n",
    "        \n",
    "        Args:\n",
    "            row (int): Fila\n",
    "            col (int): Columna\n",
    "            \n",
    "        Returns:\n",
    "            int: Número de estado\n",
    "        \"\"\"\n",
    "        return row * self.cols + col\n",
    "    \n",
    "    def is_valid_position(self, row, col):\n",
    "        \"\"\"\n",
    "        Verifica si una posición está dentro del grid.\n",
    "        \n",
    "        Args:\n",
    "            row (int): Fila\n",
    "            col (int): Columna\n",
    "            \n",
    "        Returns:\n",
    "            bool: True si la posición es válida\n",
    "        \"\"\"\n",
    "        return 0 <= row < self.rows and 0 <= col < self.cols\n",
    "    \n",
    "    def is_terminal(self, state):\n",
    "        \"\"\"\n",
    "        Verifica si un estado es terminal (IsEnd(s)).\n",
    "        \n",
    "        Estados terminales: Holes (5, 7, 11, 12) y Goal (15)\n",
    "        \n",
    "        Args:\n",
    "            state (int): Número de estado\n",
    "            \n",
    "        Returns:\n",
    "            bool: True si el estado es terminal\n",
    "        \"\"\"\n",
    "        return state in self.terminal_states\n",
    "    \n",
    "    def get_next_state(self, state, action):\n",
    "        \"\"\"\n",
    "        Calcula el estado resultante de tomar una acción desde un estado.\n",
    "        No considera estocasticidad, solo calcula el resultado determinístico.\n",
    "        \n",
    "        Si el movimiento sale del grid, el agente permanece en el mismo estado.\n",
    "        \n",
    "        Args:\n",
    "            state (int): Estado actual\n",
    "            action (int): Acción a realizar (0=Norte, 1=Sur, 2=Este, 3=Oeste)\n",
    "            \n",
    "        Returns:\n",
    "            int: Estado siguiente\n",
    "        \"\"\"\n",
    "        # Si ya estamos en un estado terminal, nos quedamos ahí\n",
    "        if self.is_terminal(state):\n",
    "            return state\n",
    "        \n",
    "        # Calcular nueva posición\n",
    "        row, col = self.state_to_position(state)\n",
    "        delta_row, delta_col = self.action_deltas[action]\n",
    "        new_row = row + delta_row\n",
    "        new_col = col + delta_col\n",
    "        \n",
    "        # Verificar si la nueva posición es válida\n",
    "        if self.is_valid_position(new_row, new_col):\n",
    "            return self.position_to_state(new_row, new_col)\n",
    "        else:\n",
    "            # Si el movimiento sale del grid, permanecemos en el estado actual\n",
    "            return state\n",
    "    \n",
    "    def get_transition_prob(self, state, action, next_state):\n",
    "        \"\"\"\n",
    "        Calcula la probabilidad de transición T(s, a, s').\n",
    "        \n",
    "        Implementa la dinámica estocástica del hielo resbaladizo:\n",
    "        - Probabilidad 1/3 de moverse en la dirección deseada\n",
    "        - Probabilidad 1/3 de moverse en cada dirección perpendicular\n",
    "        \n",
    "        Fórmula del MDP:\n",
    "        T(s, a, s') = P(s_{t+1} = s' | s_t = s, a_t = a)\n",
    "        \n",
    "        Args:\n",
    "            state (int): Estado actual s\n",
    "            action (int): Acción a\n",
    "            next_state (int): Estado siguiente s'\n",
    "            \n",
    "        Returns:\n",
    "            float: Probabilidad de transición T(s, a, s')\n",
    "        \"\"\"\n",
    "        # Si estamos en un estado terminal, la probabilidad de quedarse es 1\n",
    "        if self.is_terminal(state):\n",
    "            return 1.0 if next_state == state else 0.0\n",
    "        \n",
    "        # Obtener los estados resultantes de la acción deseada y las perpendiculares\n",
    "        intended_state = self.get_next_state(state, action)\n",
    "        perp_actions = self.perpendicular_actions[action]\n",
    "        perp_state1 = self.get_next_state(state, perp_actions[0])\n",
    "        perp_state2 = self.get_next_state(state, perp_actions[1])\n",
    "        \n",
    "        # Contar cuántas veces aparece next_state en los posibles resultados\n",
    "        # Cada aparición contribuye con probabilidad 1/3\n",
    "        possible_states = [intended_state, perp_state1, perp_state2]\n",
    "        count = possible_states.count(next_state)\n",
    "        \n",
    "        return count / 3.0\n",
    "    \n",
    "    def get_reward(self, state, action, next_state):\n",
    "        \"\"\"\n",
    "        Función de recompensa Reward(s, a, s').\n",
    "        \n",
    "        Definición de recompensas:\n",
    "        - Llegar al Goal (estado 15): +1\n",
    "        - Llegar a un Hole: 0 (el juego termina sin recompensa)\n",
    "        - Cualquier otra transición: 0\n",
    "        \n",
    "        Args:\n",
    "            state (int): Estado actual s\n",
    "            action (int): Acción a\n",
    "            next_state (int): Estado siguiente s'\n",
    "            \n",
    "        Returns:\n",
    "            float: Recompensa R(s, a, s')\n",
    "        \"\"\"\n",
    "        # Recompensa de +1 solo al alcanzar el Goal\n",
    "        if next_state == self.goal_state:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 0.0\n",
    "    \n",
    "    def get_possible_next_states(self, state, action):\n",
    "        \"\"\"\n",
    "        Obtiene todos los posibles estados siguientes al tomar una acción,\n",
    "        junto con sus probabilidades de transición.\n",
    "        \n",
    "        Útil para la implementación eficiente de Value Iteration.\n",
    "        \n",
    "        Args:\n",
    "            state (int): Estado actual\n",
    "            action (int): Acción\n",
    "            \n",
    "        Returns:\n",
    "            list: Lista de tuplas (next_state, probability, reward)\n",
    "        \"\"\"\n",
    "        if self.is_terminal(state):\n",
    "            return [(state, 1.0, 0.0)]\n",
    "        \n",
    "        # Calcular los tres posibles resultados\n",
    "        intended_state = self.get_next_state(state, action)\n",
    "        perp_actions = self.perpendicular_actions[action]\n",
    "        perp_state1 = self.get_next_state(state, perp_actions[0])\n",
    "        perp_state2 = self.get_next_state(state, perp_actions[1])\n",
    "        \n",
    "        # Agrupar estados idénticos y sumar sus probabilidades\n",
    "        state_probs = {}\n",
    "        for next_s in [intended_state, perp_state1, perp_state2]:\n",
    "            if next_s in state_probs:\n",
    "                state_probs[next_s] += 1/3.0\n",
    "            else:\n",
    "                state_probs[next_s] = 1/3.0\n",
    "        \n",
    "        # Crear lista de resultados con recompensas\n",
    "        results = []\n",
    "        for next_s, prob in state_probs.items():\n",
    "            reward = self.get_reward(state, action, next_s)\n",
    "            results.append((next_s, prob, reward))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def visualize_grid(self):\n",
    "        \"\"\"\n",
    "        Visualiza el mapa del Frozen Lake.\n",
    "        \"\"\"\n",
    "        print(\"\\nMapa del Frozen Lake (4x4):\")\n",
    "        print(\"=\"*30)\n",
    "        for i, row in enumerate(self.lake_map):\n",
    "            print(f\"  {' '.join(row)}    (estados {i*4} a {i*4+3})\")\n",
    "        print(\"=\"*30)\n",
    "        print(\"S = Start (inicio)\")\n",
    "        print(\"F = Frozen (camino seguro)\")\n",
    "        print(\"H = Hole (agujero - terminal)\")\n",
    "        print(\"G = Goal (meta - terminal, recompensa +1)\")\n",
    "        print(f\"\\nEstados terminales: {sorted(self.terminal_states)}\")\n",
    "        print(f\"Factor de descuento γ = {self.gamma}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5637a60",
   "metadata": {},
   "source": [
    "### Prueba del Modelado del MDP\n",
    "\n",
    "Verificamos que la función de transición T(s, a, s') y la función de recompensa R(s, a, s') estén correctamente implementadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513b3302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear instancia del MDP\n",
    "mdp = FrozenLakeMDP(gamma=0.9)\n",
    "\n",
    "# Visualizar el mapa\n",
    "mdp.visualize_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7b6312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de función de transición T(s, a, s')\n",
    "print(\"\\nEjemplo de Función de Transición T(s, a, s')\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Desde el estado inicial (0), intentamos ir al Sur (acción 1)\n",
    "state = 0\n",
    "action = 1  # Sur\n",
    "\n",
    "print(f\"\\nDesde estado {state}, acción: {mdp.action_names[action]} ({mdp.action_symbols[action]})\")\n",
    "print(\"\\nDinámica estocástica del hielo resbaladizo:\")\n",
    "print(f\"- Dirección deseada (Sur): estado {mdp.get_next_state(state, 1)}\")\n",
    "print(f\"- Dirección perpendicular 1 (Este): estado {mdp.get_next_state(state, 2)}\")\n",
    "print(f\"- Dirección perpendicular 2 (Oeste): estado {mdp.get_next_state(state, 3)}\")\n",
    "\n",
    "print(\"\\nProbabilidades de transición T(s, a, s'):\")\n",
    "for s_prime in range(mdp.n_states):\n",
    "    prob = mdp.get_transition_prob(state, action, s_prime)\n",
    "    if prob > 0:\n",
    "        print(f\"  T({state}, {mdp.action_names[action]}, {s_prime}) = {prob:.3f}\")\n",
    "\n",
    "# Verificar que las probabilidades suman 1\n",
    "total_prob = sum(mdp.get_transition_prob(state, action, s_prime) \n",
    "                 for s_prime in range(mdp.n_states))\n",
    "print(f\"\\nSuma de probabilidades: {total_prob:.3f} (debe ser 1.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeb405b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de función de recompensa R(s, a, s')\n",
    "print(\"\\nEjemplo de Función de Recompensa R(s, a, s')\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Transición al Goal\n",
    "print(f\"\\nLlegar al Goal (estado {mdp.goal_state}):\")\n",
    "print(f\"  R(14, Este, 15) = {mdp.get_reward(14, 2, 15)}\")\n",
    "\n",
    "# Transición a un Hole\n",
    "print(f\"\\nLlegar a un Hole (estado 5):\")\n",
    "print(f\"  R(1, Sur, 5) = {mdp.get_reward(1, 1, 5)}\")\n",
    "\n",
    "# Transición normal\n",
    "print(f\"\\nTransición normal (sin llegar a terminal):\")\n",
    "print(f\"  R(0, Este, 1) = {mdp.get_reward(0, 2, 1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429478f7",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2.2 - Algoritmo de Iteración de Valores (Value Iteration)\n",
    "\n",
    "Implementaremos el algoritmo de **Value Iteration** basado en las diapositivas de la presentación.\n",
    "\n",
    "### Ecuación de Bellman para Value Iteration\n",
    "\n",
    "La actualización iterativa se realiza mediante:\n",
    "\n",
    "$$V_{k+1}(s) = \\max_{a \\in Actions(s)} \\sum_{s'} T(s, a, s') [Reward(s, a, s') + \\gamma V_k(s')]$$\n",
    "\n",
    "Donde:\n",
    "- $V_k(s)$ es el valor del estado $s$ en la iteración $k$\n",
    "- $T(s, a, s')$ es la probabilidad de transición\n",
    "- $Reward(s, a, s')$ es la recompensa inmediata\n",
    "- $\\gamma$ es el factor de descuento (0.9 en nuestro caso)\n",
    "\n",
    "### Extracción de Política Óptima\n",
    "\n",
    "Una vez que $V$ ha convergido a $V_{opt}$, la política óptima se extrae mediante:\n",
    "\n",
    "$$\\pi_{opt}(s) = \\arg\\max_{a} Q_{opt}(s, a)$$\n",
    "\n",
    "Donde el Q-value óptimo es:\n",
    "\n",
    "$$Q_{opt}(s, a) = \\sum_{s'} T(s, a, s') [R(s, a, s') + \\gamma V_{opt}(s')]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c043e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueIteration:\n",
    "    \"\"\"\n",
    "    Implementación del algoritmo de Value Iteration para resolver un MDP.\n",
    "    \n",
    "    Basado en la Ecuación de Bellman:\n",
    "    V_{k+1}(s) = max_a Σ_{s'} T(s,a,s') [R(s,a,s') + γ V_k(s')]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, mdp, epsilon=1e-6, max_iterations=1000):\n",
    "        \"\"\"\n",
    "        Inicializa el algoritmo de Value Iteration.\n",
    "        \n",
    "        Args:\n",
    "            mdp (FrozenLakeMDP): El MDP a resolver\n",
    "            epsilon (float): Criterio de convergencia (diferencia máxima entre iteraciones)\n",
    "            max_iterations (int): Número máximo de iteraciones para evitar loops infinitos\n",
    "        \"\"\"\n",
    "        self.mdp = mdp\n",
    "        self.epsilon = epsilon\n",
    "        self.max_iterations = max_iterations\n",
    "        \n",
    "        # Inicialización: V_0(s) = 0 para todos los estados\n",
    "        # Como indica la presentación, comenzamos con valores de utilidad cero\n",
    "        self.V = np.zeros(mdp.n_states)\n",
    "        \n",
    "        # Política óptima (se calculará después de la convergencia)\n",
    "        self.policy = None\n",
    "        \n",
    "        # Historial de valores para análisis de convergencia\n",
    "        self.value_history = []\n",
    "        \n",
    "        # Número de iteraciones realizadas\n",
    "        self.iterations = 0\n",
    "    \n",
    "    def compute_q_value(self, state, action):\n",
    "        \"\"\"\n",
    "        Calcula el Q-value para un par estado-acción.\n",
    "        \n",
    "        Fórmula (de la presentación):\n",
    "        Q(s, a) = Σ_{s'} T(s, a, s') [R(s, a, s') + γ V(s')]\n",
    "        \n",
    "        Esta es la utilidad esperada de tomar la acción 'a' en el estado 's'\n",
    "        y luego seguir la política óptima.\n",
    "        \n",
    "        Args:\n",
    "            state (int): Estado actual s\n",
    "            action (int): Acción a\n",
    "            \n",
    "        Returns:\n",
    "            float: Q-value Q(s, a)\n",
    "        \"\"\"\n",
    "        # Si es un estado terminal, el Q-value es 0 (no hay acciones útiles)\n",
    "        if self.mdp.is_terminal(state):\n",
    "            return 0.0\n",
    "        \n",
    "        q_value = 0.0\n",
    "        \n",
    "        # Suma sobre todos los posibles estados siguientes s'\n",
    "        # Implementación vectorial de la fórmula de Bellman\n",
    "        transitions = self.mdp.get_possible_next_states(state, action)\n",
    "        \n",
    "        for next_state, prob, reward in transitions:\n",
    "            # T(s, a, s') * [R(s, a, s') + γ * V(s')]\n",
    "            q_value += prob * (reward + self.mdp.gamma * self.V[next_state])\n",
    "        \n",
    "        return q_value\n",
    "    \n",
    "    def value_iteration_step(self):\n",
    "        \"\"\"\n",
    "        Realiza una iteración del algoritmo de Value Iteration.\n",
    "        \n",
    "        Actualiza V_{k+1}(s) para todos los estados usando la Ecuación de Bellman:\n",
    "        V_{k+1}(s) = max_a Σ_{s'} T(s,a,s') [R(s,a,s') + γ V_k(s')]\n",
    "        \n",
    "        Returns:\n",
    "            float: Diferencia máxima |V_{k+1}(s) - V_k(s)| entre iteraciones\n",
    "        \"\"\"\n",
    "        # Crear un nuevo array para V_{k+1}\n",
    "        V_new = np.zeros(self.mdp.n_states)\n",
    "        \n",
    "        # Para cada estado s\n",
    "        for state in range(self.mdp.n_states):\n",
    "            if self.mdp.is_terminal(state):\n",
    "                # Los estados terminales mantienen valor 0\n",
    "                V_new[state] = 0.0\n",
    "            else:\n",
    "                # Calcular Q(s, a) para cada acción y tomar el máximo\n",
    "                # V(s) = max_a Q(s, a)\n",
    "                q_values = [self.compute_q_value(state, action) \n",
    "                           for action in self.mdp.actions]\n",
    "                V_new[state] = max(q_values)\n",
    "        \n",
    "        # Calcular la diferencia máxima (criterio de convergencia)\n",
    "        max_diff = np.max(np.abs(V_new - self.V))\n",
    "        \n",
    "        # Actualizar los valores\n",
    "        self.V = V_new\n",
    "        \n",
    "        return max_diff\n",
    "    \n",
    "    def solve(self, verbose=True):\n",
    "        \"\"\"\n",
    "        Ejecuta el algoritmo de Value Iteration hasta la convergencia.\n",
    "        \n",
    "        El algoritmo itera hasta que:\n",
    "        max_s |V_{k+1}(s) - V_k(s)| < ε\n",
    "        \n",
    "        Args:\n",
    "            verbose (bool): Si True, imprime información de progreso\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (V_optimal, numero_iteraciones)\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(\"Iniciando Value Iteration...\")\n",
    "            print(f\"Criterio de convergencia: ε = {self.epsilon}\")\n",
    "            print(f\"Factor de descuento: γ = {self.mdp.gamma}\")\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "        \n",
    "        for iteration in range(self.max_iterations):\n",
    "            # Guardar historial para análisis\n",
    "            self.value_history.append(self.V.copy())\n",
    "            \n",
    "            # Realizar una iteración\n",
    "            max_diff = self.value_iteration_step()\n",
    "            self.iterations = iteration + 1\n",
    "            \n",
    "            # Mostrar progreso cada 10 iteraciones\n",
    "            if verbose and (iteration + 1) % 10 == 0:\n",
    "                print(f\"Iteración {iteration + 1:3d}: max_diff = {max_diff:.6f}\")\n",
    "            \n",
    "            # Verificar convergencia\n",
    "            if max_diff < self.epsilon:\n",
    "                if verbose:\n",
    "                    print(\"=\"*60)\n",
    "                    print(f\"\\nConvergencia alcanzada en {iteration + 1} iteraciones!\")\n",
    "                    print(f\"Diferencia final: {max_diff:.8f} < ε = {self.epsilon}\")\n",
    "                break\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f\"\\nAdvertencia: Se alcanzó el máximo de iteraciones ({self.max_iterations})\")\n",
    "        \n",
    "        return self.V, self.iterations\n",
    "    \n",
    "    def extract_policy(self):\n",
    "        \"\"\"\n",
    "        Extrae la política óptima π* a partir de los valores óptimos V*.\n",
    "        \n",
    "        Fórmula (de la presentación):\n",
    "        π_{opt}(s) = argmax_a Q_{opt}(s, a)\n",
    "        \n",
    "        donde:\n",
    "        Q_{opt}(s, a) = Σ_{s'} T(s,a,s') [R(s,a,s') + γ V_{opt}(s')]\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: Array de tamaño n_states con la acción óptima para cada estado\n",
    "        \"\"\"\n",
    "        policy = np.zeros(self.mdp.n_states, dtype=int)\n",
    "        \n",
    "        for state in range(self.mdp.n_states):\n",
    "            if self.mdp.is_terminal(state):\n",
    "                # En estados terminales, la acción no importa (ponemos 0 por convención)\n",
    "                policy[state] = 0\n",
    "            else:\n",
    "                # Calcular Q(s, a) para cada acción\n",
    "                q_values = [self.compute_q_value(state, action) \n",
    "                           for action in self.mdp.actions]\n",
    "                \n",
    "                # Seleccionar la acción que maximiza el Q-value\n",
    "                # π*(s) = argmax_a Q(s, a)\n",
    "                policy[state] = np.argmax(q_values)\n",
    "        \n",
    "        self.policy = policy\n",
    "        return policy\n",
    "    \n",
    "    def print_values_grid(self):\n",
    "        \"\"\"\n",
    "        Imprime los valores V(s) en formato de grid 4x4.\n",
    "        \"\"\"\n",
    "        print(\"\\nValores V(s) óptimos (formato grid 4x4):\")\n",
    "        print(\"=\"*50)\n",
    "        for row in range(self.mdp.rows):\n",
    "            row_values = []\n",
    "            for col in range(self.mdp.cols):\n",
    "                state = self.mdp.position_to_state(row, col)\n",
    "                row_values.append(f\"{self.V[state]:6.3f}\")\n",
    "            print(\"  \" + \"  \".join(row_values))\n",
    "        print(\"=\"*50)\n",
    "    \n",
    "    def print_policy_grid(self):\n",
    "        \"\"\"\n",
    "        Imprime la política óptima π*(s) en formato de grid 4x4 con flechas.\n",
    "        \"\"\"\n",
    "        if self.policy is None:\n",
    "            print(\"Error: Primero debe extraer la política con extract_policy()\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\nPolítica Óptima π*(s) (formato grid 4x4):\")\n",
    "        print(\"=\"*50)\n",
    "        for row in range(self.mdp.rows):\n",
    "            row_symbols = []\n",
    "            for col in range(self.mdp.cols):\n",
    "                state = self.mdp.position_to_state(row, col)\n",
    "                \n",
    "                # Mostrar el tipo de celda para estados terminales\n",
    "                if state in self.mdp.hole_states:\n",
    "                    symbol = \"H\"\n",
    "                elif state == self.mdp.goal_state:\n",
    "                    symbol = \"G\"\n",
    "                else:\n",
    "                    # Mostrar la flecha de la acción óptima\n",
    "                    action = self.policy[state]\n",
    "                    symbol = self.mdp.action_symbols[action]\n",
    "                \n",
    "                row_symbols.append(f\"  {symbol}  \")\n",
    "            print(\"\".join(row_symbols))\n",
    "        print(\"=\"*50)\n",
    "        print(\"Leyenda: ↑=Norte  ↓=Sur  →=Este  ←=Oeste  H=Hole  G=Goal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60987642",
   "metadata": {},
   "source": [
    "### Ejecución del Algoritmo de Value Iteration\n",
    "\n",
    "Ahora ejecutaremos el algoritmo para encontrar la política óptima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d608d82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear instancia del algoritmo de Value Iteration\n",
    "vi = ValueIteration(mdp, epsilon=1e-6, max_iterations=1000)\n",
    "\n",
    "# Ejecutar el algoritmo hasta convergencia\n",
    "V_optimal, num_iterations = vi.solve(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85919af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar los valores óptimos V*(s)\n",
    "vi.print_values_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397deea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer y mostrar la política óptima π*(s)\n",
    "optimal_policy = vi.extract_policy()\n",
    "vi.print_policy_grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69062fb",
   "metadata": {},
   "source": [
    "---\n",
    "## Visualizaciones\n",
    "\n",
    "### 1. Mapa de Calor de los Valores V(s)\n",
    "\n",
    "Este mapa muestra qué celdas son las más valiosas. Los estados con valores más altos son aquellos desde los cuales es más probable alcanzar el Goal con buenas recompensas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709d14bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_value_heatmap(vi):\n",
    "    \"\"\"\n",
    "    Crea un mapa de calor de los valores V(s) óptimos.\n",
    "    \n",
    "    Los colores más cálidos (rojos/naranjas) indican estados más valiosos,\n",
    "    es decir, estados desde los cuales se puede alcanzar el Goal con mayor\n",
    "    utilidad esperada.\n",
    "    \"\"\"\n",
    "    # Convertir el vector de valores a matriz 4x4\n",
    "    V_grid = vi.V.reshape((vi.mdp.rows, vi.mdp.cols))\n",
    "    \n",
    "    # Crear figura\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Crear mapa de calor\n",
    "    im = ax.imshow(V_grid, cmap='YlOrRd', aspect='equal')\n",
    "    \n",
    "    # Agregar barra de color\n",
    "    cbar = plt.colorbar(im, ax=ax)\n",
    "    cbar.set_label('Valor V(s)', rotation=270, labelpad=20, fontsize=12)\n",
    "    \n",
    "    # Configurar ejes\n",
    "    ax.set_xticks(np.arange(vi.mdp.cols))\n",
    "    ax.set_yticks(np.arange(vi.mdp.rows))\n",
    "    ax.set_xticklabels(np.arange(vi.mdp.cols))\n",
    "    ax.set_yticklabels(np.arange(vi.mdp.rows))\n",
    "    \n",
    "    # Agregar grid\n",
    "    ax.set_xticks(np.arange(vi.mdp.cols + 1) - 0.5, minor=True)\n",
    "    ax.set_yticks(np.arange(vi.mdp.rows + 1) - 0.5, minor=True)\n",
    "    ax.grid(which='minor', color='gray', linestyle='-', linewidth=2)\n",
    "    \n",
    "    # Agregar valores numéricos y etiquetas de celda\n",
    "    for i in range(vi.mdp.rows):\n",
    "        for j in range(vi.mdp.cols):\n",
    "            state = vi.mdp.position_to_state(i, j)\n",
    "            cell_type = vi.mdp.lake_map[i][j]\n",
    "            \n",
    "            # Determinar color del texto según el valor del fondo\n",
    "            text_color = 'white' if V_grid[i, j] > 0.3 else 'black'\n",
    "            \n",
    "            # Agregar etiqueta de tipo de celda\n",
    "            ax.text(j, i - 0.25, cell_type, ha='center', va='center',\n",
    "                   color=text_color, fontsize=14, fontweight='bold')\n",
    "            \n",
    "            # Agregar valor numérico\n",
    "            ax.text(j, i + 0.1, f'{V_grid[i, j]:.3f}', ha='center', va='center',\n",
    "                   color=text_color, fontsize=10)\n",
    "            \n",
    "            # Agregar número de estado\n",
    "            ax.text(j, i + 0.35, f'(s={state})', ha='center', va='center',\n",
    "                   color=text_color, fontsize=8, style='italic')\n",
    "    \n",
    "    ax.set_title('Mapa de Calor de Valores V(s) Óptimos\\n' + \n",
    "                'Los estados más valiosos están en rojo/naranja',\n",
    "                fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.set_xlabel('Columna', fontsize=12)\n",
    "    ax.set_ylabel('Fila', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Análisis de los estados más valiosos\n",
    "    print(\"\\nAnálisis de Estados Más Valiosos:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Ordenar estados por valor\n",
    "    state_values = [(s, vi.V[s]) for s in range(vi.mdp.n_states)]\n",
    "    state_values.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"\\nTop 5 estados más valiosos:\")\n",
    "    for idx, (state, value) in enumerate(state_values[:5], 1):\n",
    "        row, col = vi.mdp.state_to_position(state)\n",
    "        cell_type = vi.mdp.lake_map[row][col]\n",
    "        print(f\"{idx}. Estado {state} (fila {row}, col {col}, tipo '{cell_type}'): V = {value:.4f}\")\n",
    "    \n",
    "    print(\"\\n¿Por qué estos estados son valiosos?\")\n",
    "    print(\"Los estados cerca del Goal (G) tienen valores más altos porque:\")\n",
    "    print(\"- Están más cerca de la recompensa de +1\")\n",
    "    print(\"- Con γ=0.9, las recompensas futuras cercanas valen más que las lejanas\")\n",
    "    print(\"- Tienen menor riesgo de caer en un Hole antes de alcanzar el Goal\")\n",
    "\n",
    "# Generar el mapa de calor\n",
    "plot_value_heatmap(vi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fdaa2f",
   "metadata": {},
   "source": [
    "### 2. Visualización de la Política Óptima con Flechas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154aa898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_optimal_policy(vi):\n",
    "    \"\"\"\n",
    "    Visualiza la política óptima π*(s) mostrando flechas para cada estado.\n",
    "    \n",
    "    Las flechas indican la dirección óptima a seguir desde cada estado.\n",
    "    Los estados terminales (Holes y Goal) se marcan de forma especial.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Crear una matriz de colores para el fondo\n",
    "    background = np.ones((vi.mdp.rows, vi.mdp.cols))\n",
    "    \n",
    "    # Marcar estados especiales\n",
    "    for i in range(vi.mdp.rows):\n",
    "        for j in range(vi.mdp.cols):\n",
    "            state = vi.mdp.position_to_state(i, j)\n",
    "            if state in vi.mdp.hole_states:\n",
    "                background[i, j] = 0.3  # Holes en gris oscuro\n",
    "            elif state == vi.mdp.goal_state:\n",
    "                background[i, j] = 0.7  # Goal en verde claro\n",
    "    \n",
    "    # Mostrar fondo\n",
    "    ax.imshow(background, cmap='RdYlGn', aspect='equal', alpha=0.5, vmin=0, vmax=1)\n",
    "    \n",
    "    # Configurar grid\n",
    "    ax.set_xticks(np.arange(vi.mdp.cols))\n",
    "    ax.set_yticks(np.arange(vi.mdp.rows))\n",
    "    ax.set_xticklabels(np.arange(vi.mdp.cols))\n",
    "    ax.set_yticklabels(np.arange(vi.mdp.rows))\n",
    "    \n",
    "    ax.set_xticks(np.arange(vi.mdp.cols + 1) - 0.5, minor=True)\n",
    "    ax.set_yticks(np.arange(vi.mdp.rows + 1) - 0.5, minor=True)\n",
    "    ax.grid(which='minor', color='black', linestyle='-', linewidth=2)\n",
    "    \n",
    "    # Mapeo de acciones a vectores de dirección para las flechas\n",
    "    action_to_arrow = {\n",
    "        0: (0, 0.3),    # Norte: arriba\n",
    "        1: (0, -0.3),   # Sur: abajo\n",
    "        2: (0.3, 0),    # Este: derecha\n",
    "        3: (-0.3, 0)    # Oeste: izquierda\n",
    "    }\n",
    "    \n",
    "    # Dibujar flechas y etiquetas\n",
    "    for i in range(vi.mdp.rows):\n",
    "        for j in range(vi.mdp.cols):\n",
    "            state = vi.mdp.position_to_state(i, j)\n",
    "            cell_type = vi.mdp.lake_map[i][j]\n",
    "            \n",
    "            if state in vi.mdp.hole_states:\n",
    "                # Marcar Holes\n",
    "                ax.text(j, i, 'H\\nHole', ha='center', va='center',\n",
    "                       fontsize=14, fontweight='bold', color='white')\n",
    "            elif state == vi.mdp.goal_state:\n",
    "                # Marcar Goal\n",
    "                ax.text(j, i, 'G\\nGoal', ha='center', va='center',\n",
    "                       fontsize=14, fontweight='bold', color='darkgreen')\n",
    "            else:\n",
    "                # Dibujar flecha de la acción óptima\n",
    "                action = vi.policy[state]\n",
    "                dx, dy = action_to_arrow[action]\n",
    "                \n",
    "                ax.arrow(j, i, dx, dy, head_width=0.15, head_length=0.1,\n",
    "                        fc='blue', ec='blue', linewidth=2)\n",
    "                \n",
    "                # Agregar etiqueta de acción\n",
    "                action_name = vi.mdp.action_names[action]\n",
    "                ax.text(j, i + 0.45, action_name[0], ha='center', va='center',\n",
    "                       fontsize=9, style='italic', color='darkblue')\n",
    "            \n",
    "            # Agregar número de estado en la esquina\n",
    "            ax.text(j - 0.4, i - 0.4, str(state), ha='left', va='top',\n",
    "                   fontsize=8, color='black', bbox=dict(boxstyle='round,pad=0.3',\n",
    "                   facecolor='white', alpha=0.7))\n",
    "    \n",
    "    ax.set_title('Política Óptima π*(s) - Acción Óptima para cada Estado\\n' +\n",
    "                'Las flechas indican la dirección óptima a seguir',\n",
    "                fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.set_xlabel('Columna', fontsize=12)\n",
    "    ax.set_ylabel('Fila', fontsize=12)\n",
    "    \n",
    "    # Agregar leyenda\n",
    "    legend_elements = [\n",
    "        mpatches.Patch(facecolor='lightgreen', edgecolor='black', label='Goal (G)'),\n",
    "        mpatches.Patch(facecolor='gray', edgecolor='black', label='Hole (H)'),\n",
    "        mpatches.Patch(facecolor='lightyellow', edgecolor='black', label='Frozen (F)'),\n",
    "        mpatches.FancyArrow(0, 0, 0.3, 0, color='blue', label='Acción óptima')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='upper left', bbox_to_anchor=(1.05, 1))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generar la visualización de la política\n",
    "plot_optimal_policy(vi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c454514",
   "metadata": {},
   "source": [
    "### 3. Convergencia del Algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de9f928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_convergence(vi):\n",
    "    \"\"\"\n",
    "    Visualiza la convergencia del algoritmo de Value Iteration.\n",
    "    \n",
    "    Muestra cómo los valores V(s) evolucionan a lo largo de las iteraciones\n",
    "    hasta alcanzar la convergencia.\n",
    "    \"\"\"\n",
    "    history = np.array(vi.value_history)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Gráfico 1: Evolución de valores para algunos estados clave\n",
    "    estados_clave = [0, 10, 14, 15]  # Start, centro, cerca de Goal, Goal\n",
    "    estados_nombres = ['Estado 0 (Start)', 'Estado 10 (Centro)', \n",
    "                      'Estado 14 (Cerca Goal)', 'Estado 15 (Goal)']\n",
    "    \n",
    "    for estado, nombre in zip(estados_clave, estados_nombres):\n",
    "        valores = history[:, estado]\n",
    "        ax1.plot(valores, marker='o', markersize=3, label=nombre, linewidth=2)\n",
    "    \n",
    "    ax1.set_xlabel('Iteración', fontsize=12)\n",
    "    ax1.set_ylabel('Valor V(s)', fontsize=12)\n",
    "    ax1.set_title('Convergencia de Valores V(s) por Iteración', fontsize=13, fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gráfico 2: Diferencia máxima entre iteraciones\n",
    "    if len(history) > 1:\n",
    "        max_diffs = []\n",
    "        for i in range(1, len(history)):\n",
    "            diff = np.max(np.abs(history[i] - history[i-1]))\n",
    "            max_diffs.append(diff)\n",
    "        \n",
    "        ax2.plot(max_diffs, color='red', linewidth=2, marker='o', markersize=3)\n",
    "        ax2.axhline(y=vi.epsilon, color='green', linestyle='--', linewidth=2, \n",
    "                   label=f'Umbral ε = {vi.epsilon}')\n",
    "        ax2.set_xlabel('Iteración', fontsize=12)\n",
    "        ax2.set_ylabel('max |V_{k+1}(s) - V_k(s)|', fontsize=12)\n",
    "        ax2.set_title('Criterio de Convergencia\\n' +\n",
    "                     'Diferencia Máxima entre Iteraciones', \n",
    "                     fontsize=13, fontweight='bold')\n",
    "        ax2.set_yscale('log')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nAnálisis de Convergencia:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Número total de iteraciones: {vi.iterations}\")\n",
    "    print(f\"Criterio de convergencia (ε): {vi.epsilon}\")\n",
    "    print(f\"\\nLa convergencia se alcanza cuando:\")\n",
    "    print(f\"  max_s |V_{{k+1}}(s) - V_k(s)| < ε\")\n",
    "    print(f\"\\nEsto significa que los valores han dejado de cambiar significativamente,\")\n",
    "    print(f\"indicando que hemos encontrado V* (los valores óptimos).\")\n",
    "\n",
    "# Generar la visualización de convergencia\n",
    "plot_convergence(vi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163b1e89",
   "metadata": {},
   "source": [
    "---\n",
    "## Análisis de Resultados\n",
    "\n",
    "### Interpretación de los Valores V(s)\n",
    "\n",
    "Los valores V(s) representan la **utilidad esperada** de comenzar en el estado s y seguir la política óptima. Observamos que:\n",
    "\n",
    "1. **Estados cerca del Goal tienen valores más altos:** Esto se debe a que:\n",
    "   - Están más cerca de la recompensa de +1\n",
    "   - Con γ=0.9, las recompensas futuras se descuentan, por lo que alcanzar el Goal en menos pasos vale más\n",
    "   \n",
    "2. **Estados cerca de Holes tienen valores más bajos:** El riesgo de caer en un agujero reduce la utilidad esperada.\n",
    "\n",
    "3. **La estocasticidad afecta los valores:** Debido a que el hielo es resbaladizo (solo 1/3 de probabilidad de moverse en la dirección deseada), algunos estados que parecen cercanos al Goal pueden tener valores menores si están rodeados de Holes.\n",
    "\n",
    "### Interpretación de la Política Óptima π*(s)\n",
    "\n",
    "La política óptima nos dice qué acción tomar en cada estado para maximizar la utilidad esperada. Las flechas muestran el camino óptimo considerando:\n",
    "\n",
    "1. **La estocasticidad del movimiento:** La política no siempre apunta directamente al Goal porque debe considerar el riesgo de resbalar hacia Holes.\n",
    "\n",
    "2. **Equilibrio entre riesgo y recompensa:** En algunos estados, la política óptima puede preferir un camino más largo pero más seguro en lugar de un atajo riesgoso.\n",
    "\n",
    "### Relación con las Fórmulas de Bellman\n",
    "\n",
    "Los resultados obtenidos son consecuencia directa de la **Ecuación de Bellman**:\n",
    "\n",
    "$$V_{k+1}(s) = \\max_{a \\in Actions(s)} \\sum_{s'} T(s, a, s') [Reward(s, a, s') + \\gamma V_k(s')]$$\n",
    "\n",
    "Esta ecuación:\n",
    "- Considera todas las transiciones posibles desde el estado actual\n",
    "- Pondera cada transición por su probabilidad T(s, a, s')\n",
    "- Suma la recompensa inmediata más el valor futuro descontado\n",
    "- Elige la acción que maximiza esta suma\n",
    "\n",
    "El factor de descuento γ=0.9 significa que una recompensa de +1 en el siguiente paso vale 0.9, en dos pasos vale 0.9² = 0.81, etc. Esto favorece políticas que alcancen el Goal rápidamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccf3d5f",
   "metadata": {},
   "source": [
    "---\n",
    "## Simulación de un Episodio\n",
    "\n",
    "Para validar nuestra política óptima, simularemos algunos episodios siguiendo π*(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5075aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_episode(mdp, policy, max_steps=100, seed=None):\n",
    "    \"\"\"\n",
    "    Simula un episodio en el Frozen Lake siguiendo una política dada.\n",
    "    \n",
    "    Args:\n",
    "        mdp (FrozenLakeMDP): El MDP\n",
    "        policy (numpy.ndarray): Política a seguir\n",
    "        max_steps (int): Número máximo de pasos para evitar loops infinitos\n",
    "        seed (int): Semilla para reproducibilidad\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (trayectoria, recompensa_total, exito)\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    state = mdp.start_state\n",
    "    trajectory = [state]\n",
    "    total_reward = 0.0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        if mdp.is_terminal(state):\n",
    "            break\n",
    "        \n",
    "        # Seleccionar acción según la política\n",
    "        action = policy[state]\n",
    "        \n",
    "        # Obtener transiciones posibles con sus probabilidades\n",
    "        transitions = mdp.get_possible_next_states(state, action)\n",
    "        \n",
    "        # Samplear el siguiente estado según las probabilidades\n",
    "        next_states = [t[0] for t in transitions]\n",
    "        probs = [t[1] for t in transitions]\n",
    "        rewards = [t[2] for t in transitions]\n",
    "        \n",
    "        idx = np.random.choice(len(next_states), p=probs)\n",
    "        next_state = next_states[idx]\n",
    "        reward = rewards[idx]\n",
    "        \n",
    "        # Actualizar\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        trajectory.append(state)\n",
    "    \n",
    "    success = (state == mdp.goal_state)\n",
    "    \n",
    "    return trajectory, total_reward, success\n",
    "\n",
    "def visualize_trajectory(mdp, trajectory):\n",
    "    \"\"\"\n",
    "    Visualiza una trayectoria en el grid.\n",
    "    \"\"\"\n",
    "    print(\"\\nTrayectoria del Agente:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for step, state in enumerate(trajectory):\n",
    "        row, col = mdp.state_to_position(state)\n",
    "        cell_type = mdp.lake_map[row][col]\n",
    "        print(f\"Paso {step}: Estado {state} (fila {row}, col {col}) - Tipo: {cell_type}\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Simular varios episodios\n",
    "print(\"Simulación de Episodios con la Política Óptima\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "num_episodes = 5\n",
    "successes = 0\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    trajectory, reward, success = simulate_episode(mdp, optimal_policy, seed=episode)\n",
    "    \n",
    "    print(f\"\\n--- Episodio {episode + 1} ---\")\n",
    "    print(f\"Pasos totales: {len(trajectory) - 1}\")\n",
    "    print(f\"Recompensa total: {reward}\")\n",
    "    print(f\"¿Alcanzó el Goal?: {'Sí' if success else 'No'}\")\n",
    "    \n",
    "    if success:\n",
    "        successes += 1\n",
    "    \n",
    "    visualize_trajectory(mdp, trajectory)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Tasa de éxito: {successes}/{num_episodes} = {successes/num_episodes*100:.1f}%\")\n",
    "print(f\"\\nNota: Debido a la naturaleza estocástica del hielo resbaladizo,\")\n",
    "print(f\"no todos los episodios alcanzarán el Goal, incluso con la política óptima.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3241336b",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusiones\n",
    "\n",
    "### Task 2.1 - Modelado del MDP\n",
    "\n",
    "Hemos implementado exitosamente el Frozen Lake como un Markov Decision Process, definiendo:\n",
    "\n",
    "1. **Estados (S):** 16 estados numerados de 0 a 15 en un grid 4x4\n",
    "2. **Acciones (A):** Norte, Sur, Este, Oeste\n",
    "3. **Función de transición T(s, a, s'):** Captura la estocasticidad del hielo resbaladizo con probabilidades 1/3\n",
    "4. **Función de recompensa R(s, a, s'):** +1 al alcanzar el Goal, 0 en otros casos\n",
    "5. **Estados terminales:** Holes (5, 7, 11, 12) y Goal (15)\n",
    "\n",
    "### Task 2.2 - Value Iteration\n",
    "\n",
    "Hemos implementado el algoritmo de Value Iteration siguiendo la Ecuación de Bellman:\n",
    "\n",
    "$$V_{k+1}(s) = \\max_{a} \\sum_{s'} T(s, a, s') [R(s, a, s') + \\gamma V_k(s')]$$\n",
    "\n",
    "El algoritmo:\n",
    "- Converge exitosamente a la solución óptima V*\n",
    "- Extrae la política óptima π* mediante argmax de los Q-values\n",
    "- Utiliza γ=0.9 como factor de descuento\n",
    "\n",
    "### Insights Principales\n",
    "\n",
    "1. **La estocasticidad complica la planificación:** El hielo resbaladizo requiere que la política considere movimientos no deseados, resultando en caminos que no siempre apuntan directamente al Goal.\n",
    "\n",
    "2. **El factor de descuento importa:** Con γ=0.9, el agente prefiere alcanzar el Goal rápidamente, ya que recompensas futuras valen menos.\n",
    "\n",
    "3. **Value Iteration es eficiente:** El algoritmo converge en pocas iteraciones, calculando tanto los valores óptimos como la política óptima.\n",
    "\n",
    "4. **Los valores V(s) reflejan utilidad esperada:** Estados más valiosos son aquellos desde los cuales es más probable alcanzar el Goal con buena recompensa.\n",
    "\n",
    "### Conexión con la Teoría\n",
    "\n",
    "Todo el desarrollo se basa rigurosamente en las fórmulas presentadas en clase:\n",
    "\n",
    "- La Ecuación de Bellman para actualización de valores\n",
    "- La definición formal de un MDP con sus componentes\n",
    "- El concepto de política óptima y Q-values\n",
    "- El factor de descuento γ para valorar recompensas futuras\n",
    "\n",
    "Esta implementación demuestra cómo la teoría de MDPs se aplica a problemas reales con incertidumbre, proporcionando una solución sistemática y óptima."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
