{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c04d236",
   "metadata": {},
   "source": [
    "## Universidad del Valle de Guatemala  \n",
    "## Departamento de Ciencias de la Computación  \n",
    "## Inteligencia Artificial - sección 10  \n",
    "\n",
    "### Hoja de Trabajo 2\n",
    "\n",
    "#### Nadissa Vela - 23764  \n",
    "#### Cristian Túnchez - 231359\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede6972e",
   "metadata": {},
   "source": [
    "# Task 2 - Implementación Frozen Lake en Python\n",
    "\n",
    "El agente debe cruzar un lago congelado representado por un grid de 4x4 donde:\n",
    "- **S (Start):** Posición inicial\n",
    "- **F (Frozen):** Camino seguro\n",
    "- **H (Hole):** Agujero - Termina el juego con recompensa 0\n",
    "- **G (Goal):** Meta - Recompensa +1\n",
    "\n",
    "**Dinámica Estocástica:**  \n",
    "El hielo es resbaladizo. Al tomar una acción, hay:\n",
    "- 1/3 de probabilidad de moverse en la dirección deseada\n",
    "- 1/3 de probabilidad de moverse a cada dirección perpendicular\n",
    "\n",
    "Ejemplo: Si intenta ir Norte, puede terminar en Norte, Este u Oeste con igual probabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea840ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de bibliotecas permitidas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4c036b",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2.1 - Modelado del MDP\n",
    "\n",
    "Implementaremos una clase `FrozenLakeMDP` que modele completamente el problema como un Proceso de Decisión de Markov.\n",
    "\n",
    "### Componentes del MDP\n",
    "\n",
    "Según la teoría, un MDP se define por:\n",
    "1. **Estados (S):** Conjunto de todos los estados posibles\n",
    "2. **Estado inicial (s_start):** Estado donde comienza el agente\n",
    "3. **Acciones (Actions(s)):** Acciones disponibles en cada estado\n",
    "4. **Función de transición (T(s, a, s')):** Probabilidad de llegar a s' al realizar acción a en estado s\n",
    "5. **Función de recompensa (Reward(s, a, s')):** Recompensa recibida por la transición\n",
    "6. **Estados finales (IsEnd(s)):** Indica si un estado es terminal\n",
    "7. **Factor de descuento (γ):** Importancia de recompensas futuras (usaremos γ=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5babc3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenLakeMDP:\n",
    "    \"\"\"\n",
    "    Implementación del problema Frozen Lake como un Markov Decision Process.\n",
    "    \n",
    "    Grid 4x4 con numeración de estados:\n",
    "    [ 0  1  2  3]\n",
    "    [ 4  5  6  7]\n",
    "    [ 8  9 10 11]\n",
    "    [12 13 14 15]\n",
    "    \n",
    "    Mapa del lago:\n",
    "    S F F F\n",
    "    F H F H\n",
    "    F F F H\n",
    "    H F F G\n",
    "    \n",
    "    Donde:\n",
    "    S = Start (estado 0)\n",
    "    F = Frozen (camino seguro)\n",
    "    H = Hole (agujero, estado terminal)\n",
    "    G = Goal (meta, estado terminal)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, gamma=0.9):\n",
    "        \"\"\"\n",
    "        Inicializa el MDP del Frozen Lake.\n",
    "        \n",
    "        Args:\n",
    "            gamma (float): Factor de descuento γ ∈ [0,1] para recompensas futuras\n",
    "        \"\"\"\n",
    "        # Factor de descuento γ\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Dimensiones del grid\n",
    "        self.rows = 4\n",
    "        self.cols = 4\n",
    "        self.n_states = self.rows * self.cols  # 16 estados (0-15)\n",
    "        \n",
    "        # Definición del mapa del lago (usando el layout estándar de Frozen Lake)\n",
    "        # S: Start, F: Frozen, H: Hole, G: Goal\n",
    "        self.lake_map = [\n",
    "            ['S', 'F', 'F', 'F'],\n",
    "            ['F', 'H', 'F', 'H'],\n",
    "            ['F', 'F', 'F', 'H'],\n",
    "            ['H', 'F', 'F', 'G']\n",
    "        ]\n",
    "        \n",
    "        # Estado inicial\n",
    "        self.start_state = 0  # Esquina superior izquierda\n",
    "        \n",
    "        # Estados terminales (Holes y Goal)\n",
    "        # Holes: 5, 7, 11, 12\n",
    "        # Goal: 15\n",
    "        self.hole_states = {5, 7, 11, 12}\n",
    "        self.goal_state = 15\n",
    "        self.terminal_states = self.hole_states | {self.goal_state}\n",
    "        \n",
    "        # Definición de acciones: 0=Norte, 1=Sur, 2=Este, 3=Oeste\n",
    "        self.actions = [0, 1, 2, 3]\n",
    "        self.action_names = ['Norte', 'Sur', 'Este', 'Oeste']\n",
    "        self.action_symbols = ['↑', '↓', '→', '←']\n",
    "        \n",
    "        # Movimientos correspondientes a cada acción\n",
    "        # (delta_row, delta_col)\n",
    "        self.action_deltas = {\n",
    "            0: (-1, 0),  # Norte: arriba\n",
    "            1: (1, 0),   # Sur: abajo\n",
    "            2: (0, 1),   # Este: derecha\n",
    "            3: (0, -1)   # Oeste: izquierda\n",
    "        }\n",
    "        \n",
    "        # Precalcular las direcciones perpendiculares para cada acción\n",
    "        # Para modelar la estocasticidad del hielo resbaladizo\n",
    "        self.perpendicular_actions = {\n",
    "            0: [2, 3],  # Norte -> perpendiculares son Este y Oeste\n",
    "            1: [2, 3],  # Sur -> perpendiculares son Este y Oeste\n",
    "            2: [0, 1],  # Este -> perpendiculares son Norte y Sur\n",
    "            3: [0, 1]   # Oeste -> perpendiculares son Norte y Sur\n",
    "        }\n",
    "    \n",
    "    def state_to_position(self, state):\n",
    "        \"\"\"\n",
    "        Convierte un número de estado (0-15) a posición (row, col) en el grid.\n",
    "        \n",
    "        Args:\n",
    "            state (int): Número de estado\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (row, col)\n",
    "        \"\"\"\n",
    "        row = state // self.cols\n",
    "        col = state % self.cols\n",
    "        return row, col\n",
    "    \n",
    "    def position_to_state(self, row, col):\n",
    "        \"\"\"\n",
    "        Convierte una posición (row, col) a número de estado.\n",
    "        \n",
    "        Args:\n",
    "            row (int): Fila\n",
    "            col (int): Columna\n",
    "            \n",
    "        Returns:\n",
    "            int: Número de estado\n",
    "        \"\"\"\n",
    "        return row * self.cols + col\n",
    "    \n",
    "    def is_valid_position(self, row, col):\n",
    "        \"\"\"\n",
    "        Verifica si una posición está dentro del grid.\n",
    "        \n",
    "        Args:\n",
    "            row (int): Fila\n",
    "            col (int): Columna\n",
    "            \n",
    "        Returns:\n",
    "            bool: True si la posición es válida\n",
    "        \"\"\"\n",
    "        return 0 <= row < self.rows and 0 <= col < self.cols\n",
    "    \n",
    "    def is_terminal(self, state):\n",
    "        \"\"\"\n",
    "        Verifica si un estado es terminal (IsEnd(s)).\n",
    "        \n",
    "        Estados terminales: Holes (5, 7, 11, 12) y Goal (15)\n",
    "        \n",
    "        Args:\n",
    "            state (int): Número de estado\n",
    "            \n",
    "        Returns:\n",
    "            bool: True si el estado es terminal\n",
    "        \"\"\"\n",
    "        return state in self.terminal_states\n",
    "    \n",
    "    def get_next_state(self, state, action):\n",
    "        \"\"\"\n",
    "        Calcula el estado resultante de tomar una acción desde un estado.\n",
    "        No considera estocasticidad, solo calcula el resultado determinístico.\n",
    "        \n",
    "        Si el movimiento sale del grid, el agente permanece en el mismo estado.\n",
    "        \n",
    "        Args:\n",
    "            state (int): Estado actual\n",
    "            action (int): Acción a realizar (0=Norte, 1=Sur, 2=Este, 3=Oeste)\n",
    "            \n",
    "        Returns:\n",
    "            int: Estado siguiente\n",
    "        \"\"\"\n",
    "        # Si ya estamos en un estado terminal, nos quedamos ahí\n",
    "        if self.is_terminal(state):\n",
    "            return state\n",
    "        \n",
    "        # Calcular nueva posición\n",
    "        row, col = self.state_to_position(state)\n",
    "        delta_row, delta_col = self.action_deltas[action]\n",
    "        new_row = row + delta_row\n",
    "        new_col = col + delta_col\n",
    "        \n",
    "        # Verificar si la nueva posición es válida\n",
    "        if self.is_valid_position(new_row, new_col):\n",
    "            return self.position_to_state(new_row, new_col)\n",
    "        else:\n",
    "            # Si el movimiento sale del grid, permanecemos en el estado actual\n",
    "            return state\n",
    "    \n",
    "    def get_transition_prob(self, state, action, next_state):\n",
    "        \"\"\"\n",
    "        Calcula la probabilidad de transición T(s, a, s').\n",
    "        \n",
    "        Implementa la dinámica estocástica del hielo resbaladizo:\n",
    "        - Probabilidad 1/3 de moverse en la dirección deseada\n",
    "        - Probabilidad 1/3 de moverse en cada dirección perpendicular\n",
    "        \n",
    "        Fórmula del MDP:\n",
    "        T(s, a, s') = P(s_{t+1} = s' | s_t = s, a_t = a)\n",
    "        \n",
    "        Args:\n",
    "            state (int): Estado actual s\n",
    "            action (int): Acción a\n",
    "            next_state (int): Estado siguiente s'\n",
    "            \n",
    "        Returns:\n",
    "            float: Probabilidad de transición T(s, a, s')\n",
    "        \"\"\"\n",
    "        # Si estamos en un estado terminal, la probabilidad de quedarse es 1\n",
    "        if self.is_terminal(state):\n",
    "            return 1.0 if next_state == state else 0.0\n",
    "        \n",
    "        # Obtener los estados resultantes de la acción deseada y las perpendiculares\n",
    "        intended_state = self.get_next_state(state, action)\n",
    "        perp_actions = self.perpendicular_actions[action]\n",
    "        perp_state1 = self.get_next_state(state, perp_actions[0])\n",
    "        perp_state2 = self.get_next_state(state, perp_actions[1])\n",
    "        \n",
    "        # Contar cuántas veces aparece next_state en los posibles resultados\n",
    "        # Cada aparición contribuye con probabilidad 1/3\n",
    "        possible_states = [intended_state, perp_state1, perp_state2]\n",
    "        count = possible_states.count(next_state)\n",
    "        \n",
    "        return count / 3.0\n",
    "    \n",
    "    def get_reward(self, state, action, next_state):\n",
    "        \"\"\"\n",
    "        Función de recompensa Reward(s, a, s').\n",
    "        \n",
    "        Definición de recompensas:\n",
    "        - Llegar al Goal (estado 15): +1\n",
    "        - Llegar a un Hole: 0 (el juego termina sin recompensa)\n",
    "        - Cualquier otra transición: 0\n",
    "        \n",
    "        Args:\n",
    "            state (int): Estado actual s\n",
    "            action (int): Acción a\n",
    "            next_state (int): Estado siguiente s'\n",
    "            \n",
    "        Returns:\n",
    "            float: Recompensa R(s, a, s')\n",
    "        \"\"\"\n",
    "        # Recompensa de +1 solo al alcanzar el Goal\n",
    "        if next_state == self.goal_state:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 0.0\n",
    "    \n",
    "    def get_possible_next_states(self, state, action):\n",
    "        \"\"\"\n",
    "        Obtiene todos los posibles estados siguientes al tomar una acción,\n",
    "        junto con sus probabilidades de transición.\n",
    "        \n",
    "        Útil para la implementación eficiente de Value Iteration.\n",
    "        \n",
    "        Args:\n",
    "            state (int): Estado actual\n",
    "            action (int): Acción\n",
    "            \n",
    "        Returns:\n",
    "            list: Lista de tuplas (next_state, probability, reward)\n",
    "        \"\"\"\n",
    "        if self.is_terminal(state):\n",
    "            return [(state, 1.0, 0.0)]\n",
    "        \n",
    "        # Calcular los tres posibles resultados\n",
    "        intended_state = self.get_next_state(state, action)\n",
    "        perp_actions = self.perpendicular_actions[action]\n",
    "        perp_state1 = self.get_next_state(state, perp_actions[0])\n",
    "        perp_state2 = self.get_next_state(state, perp_actions[1])\n",
    "        \n",
    "        # Agrupar estados idénticos y sumar sus probabilidades\n",
    "        state_probs = {}\n",
    "        for next_s in [intended_state, perp_state1, perp_state2]:\n",
    "            if next_s in state_probs:\n",
    "                state_probs[next_s] += 1/3.0\n",
    "            else:\n",
    "                state_probs[next_s] = 1/3.0\n",
    "        \n",
    "        # Crear lista de resultados con recompensas\n",
    "        results = []\n",
    "        for next_s, prob in state_probs.items():\n",
    "            reward = self.get_reward(state, action, next_s)\n",
    "            results.append((next_s, prob, reward))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def visualize_grid(self):\n",
    "        \"\"\"\n",
    "        Visualiza el mapa del Frozen Lake.\n",
    "        \"\"\"\n",
    "        print(\"\\nMapa del Frozen Lake (4x4):\")\n",
    "        print(\"=\"*30)\n",
    "        for i, row in enumerate(self.lake_map):\n",
    "            print(f\"  {' '.join(row)}    (estados {i*4} a {i*4+3})\")\n",
    "        print(\"=\"*30)\n",
    "        print(\"S = Start (inicio)\")\n",
    "        print(\"F = Frozen (camino seguro)\")\n",
    "        print(\"H = Hole (agujero - terminal)\")\n",
    "        print(\"G = Goal (meta - terminal, recompensa +1)\")\n",
    "        print(f\"\\nEstados terminales: {sorted(self.terminal_states)}\")\n",
    "        print(f\"Factor de descuento γ = {self.gamma}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5637a60",
   "metadata": {},
   "source": [
    "### Prueba del Modelado del MDP\n",
    "\n",
    "Verificamos que la función de transición T(s, a, s') y la función de recompensa R(s, a, s') estén correctamente implementadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513b3302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear instancia del MDP\n",
    "mdp = FrozenLakeMDP(gamma=0.9)\n",
    "\n",
    "# Visualizar el mapa\n",
    "mdp.visualize_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7b6312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de función de transición T(s, a, s')\n",
    "print(\"\\nEjemplo de Función de Transición T(s, a, s')\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Desde el estado inicial (0), intentamos ir al Sur (acción 1)\n",
    "state = 0\n",
    "action = 1  # Sur\n",
    "\n",
    "print(f\"\\nDesde estado {state}, acción: {mdp.action_names[action]} ({mdp.action_symbols[action]})\")\n",
    "print(\"\\nDinámica estocástica del hielo resbaladizo:\")\n",
    "print(f\"- Dirección deseada (Sur): estado {mdp.get_next_state(state, 1)}\")\n",
    "print(f\"- Dirección perpendicular 1 (Este): estado {mdp.get_next_state(state, 2)}\")\n",
    "print(f\"- Dirección perpendicular 2 (Oeste): estado {mdp.get_next_state(state, 3)}\")\n",
    "\n",
    "print(\"\\nProbabilidades de transición T(s, a, s'):\")\n",
    "for s_prime in range(mdp.n_states):\n",
    "    prob = mdp.get_transition_prob(state, action, s_prime)\n",
    "    if prob > 0:\n",
    "        print(f\"  T({state}, {mdp.action_names[action]}, {s_prime}) = {prob:.3f}\")\n",
    "\n",
    "# Verificar que las probabilidades suman 1\n",
    "total_prob = sum(mdp.get_transition_prob(state, action, s_prime) \n",
    "                 for s_prime in range(mdp.n_states))\n",
    "print(f\"\\nSuma de probabilidades: {total_prob:.3f} (debe ser 1.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeb405b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de función de recompensa R(s, a, s')\n",
    "print(\"\\nEjemplo de Función de Recompensa R(s, a, s')\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Transición al Goal\n",
    "print(f\"\\nLlegar al Goal (estado {mdp.goal_state}):\")\n",
    "print(f\"  R(14, Este, 15) = {mdp.get_reward(14, 2, 15)}\")\n",
    "\n",
    "# Transición a un Hole\n",
    "print(f\"\\nLlegar a un Hole (estado 5):\")\n",
    "print(f\"  R(1, Sur, 5) = {mdp.get_reward(1, 1, 5)}\")\n",
    "\n",
    "# Transición normal\n",
    "print(f\"\\nTransición normal (sin llegar a terminal):\")\n",
    "print(f\"  R(0, Este, 1) = {mdp.get_reward(0, 2, 1)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
